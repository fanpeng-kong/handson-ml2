{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fanpeng/miniconda3/envs/tf2/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. \n",
    "*Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, all weights should be sampled independently; they should not all have the\n",
    "same initial value. One important goal of sampling weights randomly is to break\n",
    "symmetry: if all the weights have the same initial value, even if that value is not\n",
    "zero, then symmetry is not broken (i.e., all neurons in a given layer are equiva‐\n",
    "lent), and backpropagation will be unable to break it. Concretely, this means that\n",
    "all the neurons in any given layer will always have the same weights. It’s like hav‐\n",
    "ing just one neuron per layer, and much slower. It is virtually impossible for such\n",
    "a configuration to converge to a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. \n",
    "*Is it OK to initialize the bias terms to 0?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is perfectly fine to initialize the bias terms to zero. Some people like to initialize\n",
    "them just like weights, and that’s okay too; it does not make much difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. \n",
    "*Name three advantages of the SELU activation function over ReLU.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It can take on negative values, so the average output of the neurons in any\n",
    "given layer is typically closer to zero than when using the ReLU activation\n",
    "function (which never outputs negative values). This helps alleviate the vanish‐\n",
    "ing gradients problem.\n",
    "\n",
    "* It always has a nonzero derivative, which avoids the dying units issue that can\n",
    "affect ReLU units.\n",
    "\n",
    "* When the conditions are right (i.e., if the model is sequential, and the weights\n",
    "are initialized using LeCun initialization, and the inputs are standardized, and\n",
    "there’s no incompatible layer or regularization, such as dropout or ℓ1 regulari‐\n",
    "zation), then the SELU activation function ensures the model is self-\n",
    "normalized, which solves the exploding/vanishing gradients problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. \n",
    "*In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SELU activation function is a good default. If you need the neural network to\n",
    "be as fast as possible, you can use one of the leaky ReLU variants instead (e.g., a\n",
    "simple leaky ReLU using the default hyperparameter value). The simplicity of the\n",
    "ReLU activation function makes it many people’s preferred option, despite the\n",
    "fact that it is generally outperformed by SELU and leaky ReLU. However, the\n",
    "ReLU activation function’s ability to output precisely zero can be useful in some\n",
    "cases (e.g., see Chapter 17). Moreover, it can sometimes benefit from optimized\n",
    "implementation as well as from hardware acceleration. The hyperbolic tangent\n",
    "(tanh) can be useful in the output layer if you need to output a number between\n",
    "–1 and 1, but nowadays it is not used much in hidden layers (except in recurrent\n",
    "nets). The logistic activation function is also useful in the output layer when you\n",
    "need to estimate a probability (e.g., for binary classification), but is rarely used in\n",
    "hidden layers (there are exceptions—for example, for the coding layer of varia‐\n",
    "tional autoencoders; see Chapter 17). Finally, the softmax activation function is\n",
    "useful in the output layer to output probabilities for mutually exclusive classes,\n",
    "but it is rarely (if ever) used in hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. \n",
    "*What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using\n",
    "an SGD optimizer, then the algorithm will likely pick up a lot of speed, hopefully\n",
    "moving roughly toward the global minimum, but its momentum will carry it\n",
    "right past the minimum. Then it will slow down and come back, accelerate again,\n",
    "overshoot again, and so on. It may oscillate this way many times before converg‐\n",
    "ing, so overall it will take much longer to converge than with a smaller momentum\n",
    "value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. \n",
    "*Name three ways you can produce a sparse model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to produce a sparse model (i.e., with most weights equal to zero) is to\n",
    "train the model normally, then zero out tiny weights. For more sparsity, you can\n",
    "apply ℓ1 regularization during training, which pushes the optimizer toward spar‐\n",
    "sity. A third option is to use the TensorFlow Model Optimization Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. \n",
    "*Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, dropout does slow down training, in general roughly by a factor of two.\n",
    "However, it has no impact on inference speed since it is only turned on during\n",
    "training. MC Dropout is exactly like dropout during training, but it is still active\n",
    "during inference, so each inference is slowed down slightly. More importantly,\n",
    "when using MC Dropout you generally want to run inference 10 times or more\n",
    "to get better predictions. This means that making predictions is slowed down by\n",
    "a factor of 10 or more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\n",
    "Practice training a deep neural network on the CIFAR10 image dataset:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but\n",
    "it’s the point of this exercise). Use He initialization and the ELU activation\n",
    "function.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 21:41:32.511223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-02-06 21:41:32.615660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-02-06 21:41:32.616158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-02-06 21:41:32.619361: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-06 21:41:32.621097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-02-06 21:41:32.621558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-02-06 21:41:32.621855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-02-06 21:41:33.808797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-02-06 21:41:33.809353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-02-06 21:41:33.809666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-02-06 21:41:33.810453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1810 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 activation='elu',\n",
    "                                 kernel_initializer='he_normal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*b. Using Nadam optimization and early stopping, train the network on the\n",
    "CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_\n",
    "data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000\n",
    "for training, 10,000 for testing) with 10 classes, so you’ll need a softmax out‐\n",
    "put layer with 10 neurons. Remember to search for the right learning rate each\n",
    "time you change the model’s architecture or hyperparameters.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-5)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint('my_cifar10_model.h5', save_best_only=True)\n",
    "run_index = 1\n",
    "run_logdir = os.path.join(os.curdir, 'my_cifar10_logs', 'run_{:03d}'.format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5a46c4c4e5d38c52\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5a46c4c4e5d38c52\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_cifar10_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 24s 14ms/step - loss: 4.1806 - accuracy: 0.1608 - val_loss: 2.1614 - val_accuracy: 0.2166\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 2.0797 - accuracy: 0.2466 - val_loss: 2.1769 - val_accuracy: 0.2202\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.9436 - accuracy: 0.2895 - val_loss: 2.1861 - val_accuracy: 0.2178\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.8564 - accuracy: 0.3224 - val_loss: 1.8606 - val_accuracy: 0.3248\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.7880 - accuracy: 0.3516 - val_loss: 1.8212 - val_accuracy: 0.3320\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.7348 - accuracy: 0.3717 - val_loss: 1.7786 - val_accuracy: 0.3562\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.6930 - accuracy: 0.3882 - val_loss: 1.6910 - val_accuracy: 0.3818\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 26s 19ms/step - loss: 1.6548 - accuracy: 0.4047 - val_loss: 1.6705 - val_accuracy: 0.4000\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.6243 - accuracy: 0.4168 - val_loss: 1.6656 - val_accuracy: 0.4000\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.6034 - accuracy: 0.4227 - val_loss: 1.7232 - val_accuracy: 0.3684\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.5779 - accuracy: 0.4306 - val_loss: 1.6600 - val_accuracy: 0.4016\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.5581 - accuracy: 0.4399 - val_loss: 1.6275 - val_accuracy: 0.4104\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.5377 - accuracy: 0.4462 - val_loss: 1.6206 - val_accuracy: 0.4174\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.5259 - accuracy: 0.4520 - val_loss: 1.5838 - val_accuracy: 0.4242\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.5095 - accuracy: 0.4578 - val_loss: 1.5694 - val_accuracy: 0.4366\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 26s 19ms/step - loss: 1.4934 - accuracy: 0.4615 - val_loss: 1.5813 - val_accuracy: 0.4330\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.4791 - accuracy: 0.4688 - val_loss: 1.5955 - val_accuracy: 0.4370\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.4689 - accuracy: 0.4713 - val_loss: 1.5854 - val_accuracy: 0.4386\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.4556 - accuracy: 0.4749 - val_loss: 1.6104 - val_accuracy: 0.4254\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.4460 - accuracy: 0.4799 - val_loss: 1.5805 - val_accuracy: 0.4424\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.4342 - accuracy: 0.4864 - val_loss: 1.5758 - val_accuracy: 0.4384\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.4229 - accuracy: 0.4882 - val_loss: 1.5373 - val_accuracy: 0.4454\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.4089 - accuracy: 0.4931 - val_loss: 1.5447 - val_accuracy: 0.4476\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.4017 - accuracy: 0.4961 - val_loss: 1.5588 - val_accuracy: 0.4490\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.3901 - accuracy: 0.4996 - val_loss: 1.5645 - val_accuracy: 0.4480\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.3792 - accuracy: 0.5050 - val_loss: 1.5688 - val_accuracy: 0.4398\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.3681 - accuracy: 0.5077 - val_loss: 1.5790 - val_accuracy: 0.4412\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.3614 - accuracy: 0.5084 - val_loss: 1.5590 - val_accuracy: 0.4432\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.3541 - accuracy: 0.5122 - val_loss: 1.5377 - val_accuracy: 0.4522\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3410 - accuracy: 0.5162 - val_loss: 1.5418 - val_accuracy: 0.4620\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.3324 - accuracy: 0.5216 - val_loss: 1.5866 - val_accuracy: 0.4398\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3234 - accuracy: 0.5235 - val_loss: 1.5250 - val_accuracy: 0.4548\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.3162 - accuracy: 0.5265 - val_loss: 1.6261 - val_accuracy: 0.4404\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.3111 - accuracy: 0.5297 - val_loss: 1.5724 - val_accuracy: 0.4568\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.3012 - accuracy: 0.5313 - val_loss: 1.5430 - val_accuracy: 0.4620\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2926 - accuracy: 0.5364 - val_loss: 1.5476 - val_accuracy: 0.4594\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.2830 - accuracy: 0.5374 - val_loss: 1.5590 - val_accuracy: 0.4602\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2771 - accuracy: 0.5417 - val_loss: 1.5573 - val_accuracy: 0.4640\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.2685 - accuracy: 0.5420 - val_loss: 1.5765 - val_accuracy: 0.4550\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2601 - accuracy: 0.5481 - val_loss: 1.5949 - val_accuracy: 0.4516\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2531 - accuracy: 0.5490 - val_loss: 1.5276 - val_accuracy: 0.4722\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2471 - accuracy: 0.5514 - val_loss: 1.5629 - val_accuracy: 0.4648\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2393 - accuracy: 0.5538 - val_loss: 1.5712 - val_accuracy: 0.4578\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2294 - accuracy: 0.5573 - val_loss: 1.6174 - val_accuracy: 0.4450\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2265 - accuracy: 0.5600 - val_loss: 1.5702 - val_accuracy: 0.4704\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2190 - accuracy: 0.5607 - val_loss: 1.5404 - val_accuracy: 0.4788\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2099 - accuracy: 0.5651 - val_loss: 1.6093 - val_accuracy: 0.4550\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2021 - accuracy: 0.5664 - val_loss: 1.5556 - val_accuracy: 0.4640\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.1930 - accuracy: 0.5718 - val_loss: 1.5634 - val_accuracy: 0.4674\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.1867 - accuracy: 0.5735 - val_loss: 1.6202 - val_accuracy: 0.4546\n",
      "Epoch 51/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.1836 - accuracy: 0.5729 - val_loss: 1.6167 - val_accuracy: 0.4526\n",
      "Epoch 52/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.1719 - accuracy: 0.5790 - val_loss: 1.5638 - val_accuracy: 0.4680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7d480d91f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 3ms/step - loss: 1.5250 - accuracy: 0.4548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5249603986740112, 0.454800009727478]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model('my_cifar10_model.h5')\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*c. Now try adding Batch Normalization and compare the learning curves: Is it\n",
    "converging faster than before? Does it produce a better model? How does it\n",
    "affect training speed?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 56s 33ms/step - loss: 1.8425 - accuracy: 0.3395 - val_loss: 1.6721 - val_accuracy: 0.4078\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 51s 36ms/step - loss: 1.6712 - accuracy: 0.4025 - val_loss: 1.5720 - val_accuracy: 0.4352\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 53s 37ms/step - loss: 1.5979 - accuracy: 0.4318 - val_loss: 1.5345 - val_accuracy: 0.4482\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 53s 38ms/step - loss: 1.5467 - accuracy: 0.4484 - val_loss: 1.4814 - val_accuracy: 0.4720\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 54s 38ms/step - loss: 1.5025 - accuracy: 0.4669 - val_loss: 1.4358 - val_accuracy: 0.4880\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 54s 39ms/step - loss: 1.4639 - accuracy: 0.4790 - val_loss: 1.4410 - val_accuracy: 0.4816\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 55s 39ms/step - loss: 1.4328 - accuracy: 0.4916 - val_loss: 1.3999 - val_accuracy: 0.4982\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 55s 39ms/step - loss: 1.4019 - accuracy: 0.5026 - val_loss: 1.3954 - val_accuracy: 0.5084\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 53s 38ms/step - loss: 1.3767 - accuracy: 0.5118 - val_loss: 1.3897 - val_accuracy: 0.5080\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 54s 39ms/step - loss: 1.3553 - accuracy: 0.5197 - val_loss: 1.3544 - val_accuracy: 0.5226\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 55s 39ms/step - loss: 1.3393 - accuracy: 0.5240 - val_loss: 1.3558 - val_accuracy: 0.5172\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 55s 39ms/step - loss: 1.3172 - accuracy: 0.5360 - val_loss: 1.3851 - val_accuracy: 0.5166\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 56s 40ms/step - loss: 1.2963 - accuracy: 0.5404 - val_loss: 1.3599 - val_accuracy: 0.5144\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 55s 39ms/step - loss: 1.2790 - accuracy: 0.5462 - val_loss: 1.3497 - val_accuracy: 0.5326\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 55s 39ms/step - loss: 1.2603 - accuracy: 0.5553 - val_loss: 1.3673 - val_accuracy: 0.5268\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 56s 40ms/step - loss: 1.2509 - accuracy: 0.5558 - val_loss: 1.3685 - val_accuracy: 0.5218\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 1.2295 - accuracy: 0.5637 - val_loss: 1.3270 - val_accuracy: 0.5318\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 49s 35ms/step - loss: 1.2160 - accuracy: 0.5709 - val_loss: 1.3646 - val_accuracy: 0.5276\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.1995 - accuracy: 0.5773 - val_loss: 1.3466 - val_accuracy: 0.5278\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.1931 - accuracy: 0.5789 - val_loss: 1.3766 - val_accuracy: 0.5200\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.1768 - accuracy: 0.5832 - val_loss: 1.3807 - val_accuracy: 0.5220\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.1591 - accuracy: 0.5898 - val_loss: 1.3296 - val_accuracy: 0.5272\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.1527 - accuracy: 0.5944 - val_loss: 1.3253 - val_accuracy: 0.5428\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.1413 - accuracy: 0.5958 - val_loss: 1.3206 - val_accuracy: 0.5442\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.1232 - accuracy: 0.6024 - val_loss: 1.3389 - val_accuracy: 0.5392\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.1123 - accuracy: 0.6075 - val_loss: 1.3574 - val_accuracy: 0.5354\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.0986 - accuracy: 0.6129 - val_loss: 1.3394 - val_accuracy: 0.5390\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 37s 26ms/step - loss: 1.0942 - accuracy: 0.6110 - val_loss: 1.3402 - val_accuracy: 0.5356\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.0821 - accuracy: 0.6176 - val_loss: 1.3344 - val_accuracy: 0.5410\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.0690 - accuracy: 0.6221 - val_loss: 1.3398 - val_accuracy: 0.5370\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.0614 - accuracy: 0.6244 - val_loss: 1.3588 - val_accuracy: 0.5372\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 35s 25ms/step - loss: 1.0505 - accuracy: 0.6289 - val_loss: 1.3698 - val_accuracy: 0.5330\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.0369 - accuracy: 0.6332 - val_loss: 1.3592 - val_accuracy: 0.5426\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 34s 24ms/step - loss: 1.0339 - accuracy: 0.6353 - val_loss: 1.3500 - val_accuracy: 0.5486\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.0220 - accuracy: 0.6377 - val_loss: 1.3718 - val_accuracy: 0.5362\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 37s 27ms/step - loss: 1.0132 - accuracy: 0.6437 - val_loss: 1.3616 - val_accuracy: 0.5408\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 0.9981 - accuracy: 0.6472 - val_loss: 1.3538 - val_accuracy: 0.5450\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 37s 26ms/step - loss: 0.9960 - accuracy: 0.6486 - val_loss: 1.3731 - val_accuracy: 0.5462\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 34s 24ms/step - loss: 0.9839 - accuracy: 0.6516 - val_loss: 1.3851 - val_accuracy: 0.5380\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 0.9760 - accuracy: 0.6562 - val_loss: 1.3791 - val_accuracy: 0.5406\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 0.9669 - accuracy: 0.6599 - val_loss: 1.3728 - val_accuracy: 0.5432\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 35s 25ms/step - loss: 0.9559 - accuracy: 0.6624 - val_loss: 1.3844 - val_accuracy: 0.5416\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 37s 26ms/step - loss: 0.9537 - accuracy: 0.6654 - val_loss: 1.3699 - val_accuracy: 0.5442\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 37s 27ms/step - loss: 0.9400 - accuracy: 0.6672 - val_loss: 1.4311 - val_accuracy: 0.5288\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 1.3206 - accuracy: 0.5442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3205736875534058, 0.5442000031471252]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer='he_normal'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('elu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_bn_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_bn_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*d. Try replacing Batch Normalization with SELU, and make the necessary adjust‐\n",
    "ements to ensure the network self-normalizes (i.e., standardize the input fea‐\n",
    "tures, use LeCun normal initialization, make sure the DNN contains only a\n",
    "sequence of dense layers, etc.).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 34s 22ms/step - loss: 1.9415 - accuracy: 0.3041 - val_loss: 1.8389 - val_accuracy: 0.3416\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.7172 - accuracy: 0.3894 - val_loss: 1.7813 - val_accuracy: 0.3446\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.6204 - accuracy: 0.4284 - val_loss: 1.6473 - val_accuracy: 0.4196\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.5495 - accuracy: 0.4554 - val_loss: 1.6194 - val_accuracy: 0.4412\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.4962 - accuracy: 0.4772 - val_loss: 1.5726 - val_accuracy: 0.4472\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.4496 - accuracy: 0.4920 - val_loss: 1.5565 - val_accuracy: 0.4498\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.4067 - accuracy: 0.5084 - val_loss: 1.5269 - val_accuracy: 0.4648\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.3666 - accuracy: 0.5207 - val_loss: 1.4675 - val_accuracy: 0.4978\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.3273 - accuracy: 0.5355 - val_loss: 1.4972 - val_accuracy: 0.4774\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.3018 - accuracy: 0.5479 - val_loss: 1.5078 - val_accuracy: 0.4832\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.2653 - accuracy: 0.5624 - val_loss: 1.5159 - val_accuracy: 0.4938\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.2426 - accuracy: 0.5700 - val_loss: 1.4870 - val_accuracy: 0.4980\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.2166 - accuracy: 0.5805 - val_loss: 1.5203 - val_accuracy: 0.4992\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.1969 - accuracy: 0.5887 - val_loss: 1.4599 - val_accuracy: 0.5160\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.1643 - accuracy: 0.5982 - val_loss: 1.4998 - val_accuracy: 0.4988\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.1476 - accuracy: 0.6064 - val_loss: 1.5295 - val_accuracy: 0.5078\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.1336 - accuracy: 0.6113 - val_loss: 1.5077 - val_accuracy: 0.5020\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0975 - accuracy: 0.6252 - val_loss: 1.4971 - val_accuracy: 0.5120\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 32s 22ms/step - loss: 1.0765 - accuracy: 0.6325 - val_loss: 1.5312 - val_accuracy: 0.5032\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 33s 23ms/step - loss: 1.0612 - accuracy: 0.6373 - val_loss: 1.5394 - val_accuracy: 0.5000\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 34s 24ms/step - loss: 1.0503 - accuracy: 0.6451 - val_loss: 1.5725 - val_accuracy: 0.5082\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 33s 23ms/step - loss: 5.6843 - accuracy: 0.5642 - val_loss: 1.6343 - val_accuracy: 0.4602\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.2618 - accuracy: 0.5622 - val_loss: 1.5374 - val_accuracy: 0.4688\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.1720 - accuracy: 0.5942 - val_loss: 1.5027 - val_accuracy: 0.5036\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.1251 - accuracy: 0.6126 - val_loss: 1.5234 - val_accuracy: 0.5000\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0935 - accuracy: 0.6244 - val_loss: 1.5357 - val_accuracy: 0.4966\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0704 - accuracy: 0.6320 - val_loss: 1.5211 - val_accuracy: 0.4966\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.0464 - accuracy: 0.6419 - val_loss: 1.5113 - val_accuracy: 0.4936\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.0251 - accuracy: 0.6498 - val_loss: 1.5290 - val_accuracy: 0.5150\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 32s 22ms/step - loss: 1.0051 - accuracy: 0.6578 - val_loss: 1.5473 - val_accuracy: 0.5074\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 34s 24ms/step - loss: 0.9862 - accuracy: 0.6628 - val_loss: 1.5424 - val_accuracy: 0.5132\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 0.9716 - accuracy: 0.6691 - val_loss: 1.5809 - val_accuracy: 0.5052\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 0.9558 - accuracy: 0.6770 - val_loss: 1.6381 - val_accuracy: 0.5020\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 0.9489 - accuracy: 0.6776 - val_loss: 1.5475 - val_accuracy: 0.5120\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.4599 - accuracy: 0.5160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.459917426109314, 0.515999972820282]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, \n",
    "                                 kernel_initializer='lecun_normal',\n",
    "                                 activation='selu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=7e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_selu_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*e. Try regularizing the model with alpha dropout. Then, without retraining your\n",
    "model, see if you can achieve better accuracy using MC Dropout.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 32s 20ms/step - loss: 1.8903 - accuracy: 0.3285 - val_loss: 1.7161 - val_accuracy: 0.4006\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.6582 - accuracy: 0.4135 - val_loss: 1.6372 - val_accuracy: 0.4164\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 35s 25ms/step - loss: 1.5671 - accuracy: 0.4477 - val_loss: 1.6060 - val_accuracy: 0.4336\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 33s 23ms/step - loss: 1.4998 - accuracy: 0.4745 - val_loss: 1.5823 - val_accuracy: 0.4560\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.4437 - accuracy: 0.4974 - val_loss: 1.5551 - val_accuracy: 0.4670\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.3959 - accuracy: 0.5129 - val_loss: 1.5611 - val_accuracy: 0.4550\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 33s 24ms/step - loss: 1.3549 - accuracy: 0.5285 - val_loss: 1.5532 - val_accuracy: 0.4812\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 35s 25ms/step - loss: 1.3116 - accuracy: 0.5442 - val_loss: 1.4977 - val_accuracy: 0.4850\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 33s 24ms/step - loss: 1.2783 - accuracy: 0.5548 - val_loss: 1.5238 - val_accuracy: 0.4852\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 35s 25ms/step - loss: 1.2447 - accuracy: 0.5666 - val_loss: 1.5608 - val_accuracy: 0.4920\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 37s 27ms/step - loss: 1.2145 - accuracy: 0.5791 - val_loss: 1.6309 - val_accuracy: 0.4956\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 37s 26ms/step - loss: 1.1851 - accuracy: 0.5900 - val_loss: 1.6075 - val_accuracy: 0.4810\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.1549 - accuracy: 0.6020 - val_loss: 1.5785 - val_accuracy: 0.4990\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.1319 - accuracy: 0.6090 - val_loss: 1.5584 - val_accuracy: 0.5050\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.1031 - accuracy: 0.6188 - val_loss: 1.6596 - val_accuracy: 0.5108\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 33s 23ms/step - loss: 1.0814 - accuracy: 0.6274 - val_loss: 1.6529 - val_accuracy: 0.5056\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 34s 24ms/step - loss: 1.0536 - accuracy: 0.6368 - val_loss: 1.6457 - val_accuracy: 0.5122\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 35s 25ms/step - loss: 1.0343 - accuracy: 0.6443 - val_loss: 1.6647 - val_accuracy: 0.5186\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 34s 25ms/step - loss: 1.0127 - accuracy: 0.6548 - val_loss: 1.7907 - val_accuracy: 0.5084\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 34s 24ms/step - loss: 0.9915 - accuracy: 0.6616 - val_loss: 1.8256 - val_accuracy: 0.4956\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 35s 25ms/step - loss: 0.9721 - accuracy: 0.6672 - val_loss: 1.7493 - val_accuracy: 0.5094\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 0.9522 - accuracy: 0.6732 - val_loss: 1.7836 - val_accuracy: 0.5104\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 0.9362 - accuracy: 0.6807 - val_loss: 1.6934 - val_accuracy: 0.5066\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 32s 22ms/step - loss: 0.9207 - accuracy: 0.6830 - val_loss: 1.8009 - val_accuracy: 0.5118\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 33s 23ms/step - loss: 0.8941 - accuracy: 0.6952 - val_loss: 1.8588 - val_accuracy: 0.5172\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 33s 24ms/step - loss: 0.8797 - accuracy: 0.7031 - val_loss: 1.7897 - val_accuracy: 0.4874\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 34s 24ms/step - loss: 0.8610 - accuracy: 0.7092 - val_loss: 1.7873 - val_accuracy: 0.5064\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 35s 25ms/step - loss: 0.8585 - accuracy: 0.7109 - val_loss: 1.8238 - val_accuracy: 0.5134\n",
      "157/157 [==============================] - 2s 7ms/step - loss: 1.4977 - accuracy: 0.4850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4977052211761475, 0.48500001430511475]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_alpha_dropout_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_alpha_dropout_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_alpha_dropout_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model reaches 48.9% accuracy on the validation set. That's very slightly better than without dropout (47.6%). With an extensive hyperparameter search, it might be possible to do better (I tried dropout rates of 5%, 10%, 20% and 40%, and learning rates 1e-4, 3e-4, 5e-4, and 1e-3), but probably not much better in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use MC Dropout now. We will need the `MCAlphaDropout` class we used earlier, so let's just copy it here for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a new model, identical to the one we just trained (with the same weights), but with `MCAlphaDropout` dropout layers instead of `AlphaDropout` layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's add a couple utility functions. The first will run the model many times (10 by default) and it will return the mean predicted class probabilities. The second will use these mean probabilities to predict the most likely class for each instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make predictions for all the instances in the validation set, and compute the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4842"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get no accuracy improvement in this case (we're still at 48.9% accuracy).\n",
    "\n",
    "So the best model we got in this exercise is the Batch Normalization model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*f. Retrain your model using 1cycle scheduling and see if it improves training\n",
    "speed and model accuracy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 5s 12ms/step - loss: nan - accuracy: 0.1381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.999999747378752e-06,\n",
       " 9.615227699279785,\n",
       " 2.6116039752960205,\n",
       " 4.002283300672259)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train_scaled) / batch_size) * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One cycle allowed us to train the model in just 15 epochs, each taking only 2 seconds (thanks to the larger batch size). This is several times faster than the fastest model we trained so far. Moreover, we improved the model's performance (from 47.6% to 52.0%). The batch normalized model reaches a slightly better performance (54%), but it's much slower to train."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c01faa304b067634e6649e845c90d29a28bbb084674f774f8c946c047d774b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.15 ('tf2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
